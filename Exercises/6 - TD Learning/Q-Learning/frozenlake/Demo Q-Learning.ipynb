{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states:  16\n",
      "Number of actions per state:  4\n",
      "Number of episodes to run:  200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "'''\n",
    "Intuitive video about sparse rewards:\n",
    "    <https://www.youtube.com/watch?v=0Ey02HT_1Ho>\n",
    "'''\n",
    "\n",
    "# Environment\n",
    "\n",
    "import gym\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "EPISODES = 200\n",
    "\n",
    "total_states = env.observation_space.n\n",
    "actions_per_state = env.action_space.n\n",
    "print('Number of states: ', total_states)\n",
    "print('Number of actions per state: ', actions_per_state)\n",
    "print('Number of episodes to run: ', EPISODES)\n",
    "\n",
    "actions = ['Left', 'Down', 'Right', 'Up']\n",
    "states = ['S',' ',' ',' ', 'F','H','F','H','F','F','F','H','H','F','F','G']\n",
    "world = [['S',' ',' ',' '],\n",
    "         [' ',' ',' ','H'],\n",
    "         [' ',' ',' ','H'],\n",
    "         ['H',' ',' ','G']]\n",
    "\n",
    "\n",
    "# Helper function to track movement\n",
    "def draw_world(w, s):\n",
    "    i = int(np.floor(s / 4))\n",
    "    j = s % 4\n",
    "    w[i,j] = 'X'\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Table\n",
    "\n",
    "q = np.zeros((total_states, actions_per_state), dtype='float')\n",
    "Q = pd.DataFrame(q, columns=actions)\n",
    "Q.reset_index(inplace=True)\n",
    "Q.rename(columns={'index':'State'}, inplace=True)\n",
    "Q.set_index('State', inplace=True)\n",
    "\n",
    "# Helper functions to draw, update and get values of the table\n",
    "def draw_Table(Q):\n",
    "    table = sns.heatmap(Q, cmap='Blues', annot=True, linewidths=.5, cbar=False, \n",
    "                linecolor='black', square=True)\n",
    "    return table  \n",
    "\n",
    "# Helper function to sincronize the pandas DataFrame (Q) with the numpy array (q)\n",
    "def set_Q(q): \n",
    "    global Q\n",
    "    Q.iloc[:,:] = q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning Algorithm\n",
    "\n",
    "lr = .8\n",
    "y = .9\n",
    "rList = []\n",
    "\n",
    "    \n",
    "plt.ion()\n",
    "plt.figure(figsize = (10,10)).suptitle('Q-Table')\n",
    "previous = draw_Table(Q)\n",
    "\n",
    "for i in range(EPISODES):\n",
    "    \n",
    "    j = 0\n",
    "    s = env.reset()\n",
    "\n",
    "    print('Episode [{}/{}]'.format(i,EPISODES))\n",
    "    \n",
    "    rAll = 0\n",
    "    end_state = False\n",
    "    \n",
    "    while j < 99:\n",
    "        \n",
    "        j+=1\n",
    "        # Print slowly the first steps and then go fast\n",
    "        exp = np.exp(-j*(i+1)) \n",
    "        \n",
    "        # Choose action greedily\n",
    "        a = np.argmax(q[s,:] + np.random.randn(1, env.action_space.n) * (1./ (i+1)))\n",
    "        \n",
    "        # Collect reward and reach new state\n",
    "        s1,r,end_state,inf = env.step(a)\n",
    "        \n",
    "        '''\n",
    "        In order to make it visual, we are giving a reward here just by making one action.\n",
    "        If not, the values of Q will not be updated until the agent actually reached goal.\n",
    "        Therefore, r+=1 should be remove in the real implementation.\n",
    "        '''\n",
    "        r += 1\n",
    "        \n",
    "        # Update Q-Table with new knowledge\n",
    "        q[s,a] = round(q[s,a] + lr*(r + y*np.max(q[s1,:]) - q[s,a]), 2)\n",
    "        \n",
    "        # Update the dataframe. We are playing with the numpy array for easy the notation\n",
    "        set_Q(q)\n",
    "        \n",
    "        # Update the new Q-Table visualization\n",
    "        if 'previous' in globals(): \n",
    "            previous.axes.clear()\n",
    "        previous = draw_Table(Q).set_title('Episode: {}'.format(i+1))\n",
    "        plt.pause(1 * exp)\n",
    "        \n",
    "        # Rules to save figs to make later GIF        \n",
    "        if i < 10 or ((not i < 10 and i % 10 ==0) and j == 0):\n",
    "            plt.savefig('imgs/episode_{}_iter{}'.format(i,j))\n",
    "        \n",
    "        # Accumulate reward\n",
    "        rAll += r\n",
    "        \n",
    "        # Move to next state\n",
    "        s = s1\n",
    "        \n",
    "        if end_state == True: \n",
    "            break        \n",
    "\n",
    "    rList.append(rAll)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then I created a GIF with the images generated:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gif](./imgs/output.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
